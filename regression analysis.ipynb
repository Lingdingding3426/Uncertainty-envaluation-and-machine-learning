{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question 1\n",
    "## a) proof \n",
    "$$var[X] = E[(X- E[X])^2]$$\\\n",
    "$$var[Y] = E[(Y- E[Y])^2]$$\\\n",
    "$$cov[X,Y] = E((X-E[X])(Y-E[Y]))$$\\\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "var[X+Y]&=E[(X+Y- E[X+Y])^2]\\\\\n",
    "&=E[(X+Y- E[X]-E[Y])^2]\\\\\n",
    "&=E[(X-E[X]+Y-E[Y])^2]\\\\\n",
    "&=E[(X-E[X])^2+(Y-E[Y])^2+2(X-E[X])(Y-E[Y])]\\\\\n",
    "&=E[(X-E[X])^2]+E[(Y-E[Y])^2]+2E[(X-E[X])(Y-E[Y])]\\\\\n",
    "&=var[X]+var[Y]+2cov[X,Y]\n",
    "\\end{split}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x's mean is 0.14098805478962687\n",
      "y's mean is 0.3777102705654734\n",
      "x's variance is 0.9477808072935915\n",
      "y's variance is 10.32152931744173\n",
      "x,y's covariance is [array([0.95735435, 2.99696691]), array([ 2.99696691, 10.42578719])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean = [0,0]\n",
    "cov = [[1,3],[3,10]]\n",
    "x,y= np.random.multivariate_normal(mean,cov,100).T\n",
    "x_mean = np.mean(x)\n",
    "print(\"x's mean is\", x_mean)\n",
    "y_mean = np.mean(y)\n",
    "print(\"y's mean is\", y_mean)\n",
    "x_var = np.var(x)\n",
    "print(\"x's variance is\", x_var)\n",
    "y_var = np.var(y)\n",
    "print(\"y's variance is\", y_var)\n",
    "xy_cov = list(np.cov(x,y))\n",
    "print(\"x,y's covariance is\",xy_cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the result, we can see that the empirical variance and covariances is close to the initial values, so the formula is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## a)\n",
    "The likelihood is $p(x|\\theta) = \\theta e^{-\\theta x}$\\\n",
    "Therefore,after n trials\n",
    "$$ p(data|\\theta) = p(x_{1},x_{2},x_{3},...,x_{n}|\\theta) = \\theta^{n}e^{\\theta\\sum_{i=1}^{n}x_{n}}$$\n",
    "Maximizing likelihood is the same as maximizing log likelihood.\n",
    "$$log\\ likelihood = ln(p(data|\\theta)) = nln\\theta - \\theta \\sum x_{n}$$\n",
    "$$\\frac{d}{dp}(log\\ likelihood) = \\frac{n}{\\theta} - \\sum x_{n} = 0$$\n",
    "$$\\Rightarrow \\hat{\\theta} = \\frac{n}{\\sum x_{n}}$$\n",
    "$$\\Rightarrow \\hat{\\theta} = \\frac{1}{\\bar{x}}$$\n",
    "## bï¼‰\n",
    "we have $x_{1} = 5$,$x_{2} = 6$,$x_{3} = 4$\n",
    "therefore, the maximum likelihood of this data is \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\theta} &= \\frac{1}{\\bar{x}}\\\\\n",
    "&= \\frac{1}{5}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "## c)\n",
    "$\\because prior = \\theta^{3}e^{-3\\theta}$ and $likelihood = p(data|\\theta) = \\theta^{n}e^{\\theta\\sum_{i=1}^{n}x_{n}}$\\\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "posterior &= \\frac{p(\\theta)*p(x|\\theta)}{p(x)}\\\\\n",
    "    &=\\frac{\\theta^{3}e^{-3\\theta} * \\theta^{n}e^{\\theta\\sum_{i=1}^{n}x_{n}}}{\\int \\theta^{3}e^{-3\\theta} * \\theta^{n}e^{\\theta\\sum_{i=1}^{n}x_{n}}}d\\theta\\\\\n",
    "    &=c\\theta^{3}e^{-3\\theta} * \\theta^{n}e^{\\theta\\sum_{i=1}^{n}x_{n}} \\\\\n",
    "    &=c\\theta^{3+n}e^{-(3+\\theta\\sum_{i=1}^{n}x_{n})}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "$$\n",
    "## d)\n",
    "$\\because$ from c) we can see that posterior result from exponential prior and likelihood is also in the exponential family.\\\n",
    "$\\therefore$ the exponential prior conjugate to the exponential likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "I devide the dataset half into the training set and half into the test set.\\\n",
    "In total we have ten data in each variable, therefore I chose 5 as training data and another 5 as test data.\\\n",
    "I can used the model trained to predict the test data, and calculate the r-squared and adjusted r-squared.\\\n",
    "In both linear regression and ridge regression test error is larger than training error.\\\n",
    "Compared with linear regression method, ridge regression has lower test error, because ridge regression used \n",
    "regularization to solve over fitting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression R2 is 1.4744478460581565 Adjused R2 is 1.632597128077542\n",
      "Linear regression R2_train is 0.6431105600188448 Adjused R2 train is 0.5241474133584598\n",
      "Ridge regression R2 is 1.4693505820306294 Adjused R2 is 1.632597128077542\n",
      "Ridge regression R2_train is 0.6264798422947122 Adjused R2 train is 0.5241474133584598\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "x = np.array([10.9, 12.4, 13.5, 14.6, 14.8, 15.6, 16.2, 17.5, 18.3, 18.6])\n",
    "y = np.array([24.8, 30.0, 31.0, 29.3, 35.9, 36.9, 42.5, 37.9, 38.9, 40.5])\n",
    "x_train = x[:5].reshape(-1,1)\n",
    "y_train = y[:5]\n",
    "x_test = x[5:].reshape(-1,1)\n",
    "y_test = y[5:]\n",
    "n = 5\n",
    "p = 1\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(x_train, y_train)\n",
    "y_pred = regr.predict(x_test)\n",
    "y_pred_train = regr.predict(x_train)\n",
    "R2 =abs(r2_score(y_test,y_pred))\n",
    "R2_train =abs(r2_score(y_train,y_pred_train))\n",
    "Adjust_R2 = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "Adjust_R2_train = 1-(1-R2_train)*(n-1)/(n-p-1)\n",
    "print(\"Linear regression\", \"R2 is\", R2, \"Adjused R2 is\", Adjust_R2)\n",
    "print(\"Linear regression\", \"R2_train is\", R2_train, \"Adjused R2 train is\", Adjust_R2_train)\n",
    "\n",
    "n_alphas = 10\n",
    "alpha = np.logspace(-10, -2, n_alphas)\n",
    "ridge = linear_model.Ridge(alpha=0.5, fit_intercept=False)\n",
    "ridge.fit(x_train, y_train)\n",
    "\n",
    "y_pred_ridge = ridge.predict(x_test)\n",
    "y_pred_ridge_train = ridge.predict(x_train)\n",
    "R2_ridge = abs(r2_score(y_test,y_pred_ridge))\n",
    "R2_ridge_train = abs(r2_score(y_train,y_pred_ridge_train))\n",
    "Adjust_R2_ridge = 1-(1-R2)*(n-1)/(n-p-1)\n",
    "Adjust_R2_ridge_train = 1-(1-R2_train)*(n-1)/(n-p-1)\n",
    "print(\"Ridge regression\", \"R2 is\", R2_ridge, \"Adjused R2 is\", Adjust_R2_ridge)\n",
    "print(\"Ridge regression\", \"R2_train is\", R2_ridge_train, \"Adjused R2 train is\", Adjust_R2_ridge_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "The posterior is a beta distribution with 42 and 12 as variable. \\\n",
    "So we can calculate the variance according to the formular:\\\n",
    "$$var = \\frac{(\\alpha * \\beta)^{2}}{2(\\alpha + \\beta +1)}$$\n",
    "$$se = \\sqrt{var}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard error is 0.05605833101216812\n",
      "The probability of next data point to be equal one is 0.7777777777777777\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "a = 42  \n",
    "b = 12\n",
    "x = np.linspace(0.001, .999, 100)\n",
    "rv = beta(a,b)\n",
    "ax.plot(x,beta.pdf(x,a,b),'r-')\n",
    "\n",
    "mean, var, skew, kurt = beta.stats(a, b, moments='mvsk')\n",
    "se_rv = np.sqrt(var)\n",
    "print(\"standard error is\",se_rv)\n",
    "x2 = lambda x: x*beta.pdf(x,a,b)\n",
    "print(\"The probability of next data point to be equal one is\",integrate.quad(x2,0,1)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

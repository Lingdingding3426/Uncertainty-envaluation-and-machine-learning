{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## The Contraceptive Use Data (L)\n",
    "This is the alternative version of the contraceptive use data, showing the distribution of 1607 currently married and fecund women interviewed in the Fiji Fertility Survey, according to age, education, desire for more children and current use of contraception.\n",
    "\n",
    "This version has 32 rows corresponding to all possible covariate and response patterns, and includes a weight indicating the frequency of each combination. \n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{\\text { The file has 5 columns with numeric codes: }} \\\\ \n",
    "{\\cdot \\text { age (four groups, 1=<25, 2=25-29, 3=30-39 and 4=40-49) }} \\\\ \n",
    "{\\cdot \\text { education (0=none, 1=some) }} \\\\\n",
    "{\\cdot \\text { desire for more children (0=more, 1=no more) }}\\\\\n",
    "{\\cdot \\text { contraceptive use (0=no, 1=yes) }} \\\\\n",
    "{\\cdot \\text { frequency (number of cases in this category) }}\n",
    "\\end{array} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose age, education, desire for more children as features and contraceptive use as reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  cuse  desire  educ\n",
      "0       1     0       0     0\n",
      "1       1     1       0     0\n",
      "2       1     0       1     0\n",
      "3       1     1       1     0\n",
      "4       1     0       0     1\n",
      "5       1     1       0     1\n",
      "6       1     0       1     1\n",
      "7       1     1       1     1\n",
      "8       2     0       0     0\n",
      "9       2     1       0     0\n",
      "10      2     0       1     0\n",
      "11      2     1       1     0\n",
      "12      2     0       0     1\n",
      "13      2     1       0     1\n",
      "14      2     0       1     1\n",
      "15      2     1       1     1\n",
      "16      3     0       0     0\n",
      "17      3     1       0     0\n",
      "18      3     0       1     0\n",
      "19      3     1       1     0\n",
      "20      3     0       0     1\n",
      "21      3     1       0     1\n",
      "22      3     0       1     1\n",
      "23      3     1       1     1\n",
      "24      4     0       0     0\n",
      "25      4     1       0     0\n",
      "26      4     0       1     0\n",
      "27      4     1       1     0\n",
      "28      4     0       0     1\n",
      "29      4     1       0     1\n",
      "...   ...   ...     ...   ...\n",
      "1577    1     1       1     4\n",
      "1578    1     1       1     4\n",
      "1579    1     1       1     4\n",
      "1580    1     1       1     4\n",
      "1581    1     1       1     4\n",
      "1582    1     1       1     4\n",
      "1583    1     1       1     4\n",
      "1584    1     1       1     4\n",
      "1585    1     1       1     4\n",
      "1586    1     1       1     4\n",
      "1587    1     1       1     4\n",
      "1588    1     1       1     4\n",
      "1589    1     1       1     4\n",
      "1590    1     1       1     4\n",
      "1591    1     1       1     4\n",
      "1592    1     1       1     4\n",
      "1593    1     1       1     4\n",
      "1594    1     1       1     4\n",
      "1595    1     1       1     4\n",
      "1596    1     1       1     4\n",
      "1597    1     1       1     4\n",
      "1598    1     1       1     4\n",
      "1599    1     1       1     4\n",
      "1600    1     1       1     4\n",
      "1601    1     1       1     4\n",
      "1602    1     1       1     4\n",
      "1603    1     1       1     4\n",
      "1604    1     1       1     4\n",
      "1605    1     1       1     4\n",
      "1606    1     1       1     4\n",
      "\n",
      "[1607 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_stata('cuse.dta')\n",
    "\n",
    "df['educ'] = df['educ'].map({'None': 0, 'Some': 1})\n",
    "df['desire'] = df['desire'].map({'Wants no more': 1, 'Wants more': 0})\n",
    "df['age'] = df['age'].map({'<25': 1,'25-29' : 2, '30-39':3, '40-49' : 4})\n",
    "df['cuse'] = df['cuse'].map({'No': 0 , 'Yes':1})\n",
    "\n",
    "n = df['n']\n",
    "n = n.astype(int)\n",
    "df = df.drop(['n'], axis = 1)\n",
    "df1 = np.array(df)\n",
    "for i,j in enumerate(n):\n",
    "    df2 = pd.DataFrame([df1[i]] * (j-1), columns=['educ', 'desire', 'age', 'cuse'])\n",
    "    df = df.append(df2, ignore_index=True)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used sklearn package to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[414,  26],\n",
       "       [169,  34]], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = ['age', 'educ','desire']\n",
    "\n",
    "x = df[feature_cols]\n",
    "y = df['cuse']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.4, random_state = 0)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(x_test)\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.\\\n",
    "In classification question instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}{J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\operatorname{cost}\\left(h_{\\theta}\\left(x^{(i)}\\right), y^{(i)}\\right)} & {} \\\\ {\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(h_{\\theta}(x)\\right)} & {\\text { if } y=1} \\\\ {\\operatorname{cost}\\left(h_{\\theta}(x), y\\right)=-\\log \\left(1-h_{\\theta}(x)\\right)} & {\\text { if } y=0}\\end{array}\n",
    "$$\n",
    "Above functions compressed into one\n",
    "$$\n",
    "J(\\theta)=-\\frac{1}{m} \\sum_{i=1}^{m}\\left[y^{(i)} \\log \\left(h_{\\theta}\\left(x^{(i)}\\right)\\right)+\\left(1-y^{(i)}\\right) \\log \\left(1-h_{\\theta}\\left(x^{(i)}\\right)\\right)\\right]\n",
    "$$\n",
    "Vectorized cost function\\\n",
    "$$\n",
    "\\begin{array}{l}{h=g(X \\theta)} \\\\ {J(\\theta)=\\frac{1}{m} \\cdot\\left(-y^{T} \\log (h)-(1-y)^{T} \\log (1-h)\\right)}\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))\n",
    "\n",
    "def predict(features, weights):\n",
    "    z = np.dot(features, weights)\n",
    "#     print(sigmoid(z).shape)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def cost_function(features, labels, weights):\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions)\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class2_cost = (1-labels)*np.log(1-predictions)\n",
    "\n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum() / observations\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For first-order gradient desent method, the derivation of the cost function is:\n",
    "$$\n",
    "C^{\\prime}=x(s(z)-y)\n",
    "$$\n",
    "we update the weights:\n",
    "$$\n",
    "\\vec{w}_{n+1}=\\vec{w}_{n}-lr \\nabla f\\left(\\vec{w}_{n}\\right)\n",
    "$$\n",
    "\n",
    "For second-order gradient desent method(Newton's method)\n",
    "$$\n",
    "H_{\\ell(\\hat{\\theta})}=\\left[\\begin{array}{c}{\\frac{\\partial^{2} \\ell}{\\partial \\theta_{1}^{2}} \\frac{\\partial^{2} \\ell}{\\partial \\theta_{1} \\partial \\theta_{2}}} \\\\ {\\frac{\\partial^{2} \\ell}{\\partial \\theta_{2} \\partial \\theta_{1}} \\frac{\\partial^{2} \\ell}{\\partial \\theta_{2}^{2}}}\\end{array}\\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla \\ell=\\left\\langle\\begin{array}{c}{\\sum_{i=1}^{n}\\left(y_{i}-h_{\\theta}\\left(x_{i}\\right)\\right) x_{i}} \\\\ {\\sum_{i=1}^{n}\\left(y_{i}-h_{\\theta}\\left(x_{i}\\right)\\right)}\\end{array}\\right\\rangle\n",
    "$$\n",
    "$$\n",
    "H_{\\ell(\\hat{\\theta})}=\\left[\\begin{array}{c}{\\sum_{i=1}^{n} h_{\\theta}\\left(x_{i}\\right)\\left(1-h_{\\theta}\\left(x_{i}\\right)\\right) \\theta_{1} \\theta_{1}, \\sum_{i=1}^{n} h_{\\theta}\\left(x_{i}\\right)\\left(1-h_{\\theta}\\left(x_{i}\\right)\\right) \\theta_{1}} \\\\ {\\sum_{i=1}^{n} h_{\\theta}\\left(x_{i}\\right)\\left(1-h_{\\theta}\\left(x_{i}\\right)\\right) \\theta_{1}, \\sum_{i=1}^{n} h_{\\theta}\\left(x_{i}\\right)\\left(1-h_{\\theta}\\left(x_{i}\\right)\\right)}\\end{array}\\right]\n",
    "$$\n",
    "By substituting The Hessian into the Newtonâ€™s Method update step, we are left with:\n",
    "$$\n",
    "\\theta_{n+1}=\\theta_{n}+H_{\\ell(\\hat{\\theta})}^{-1} \\nabla \\ell(\\theta)\n",
    "$$\n",
    "We update the weights:\n",
    "$$\n",
    "\\vec{w}_{n+1}=\\vec{w}_{n}-H^{-1}\\left(w_{n}\\right) \\nabla f\\left(\\vec{w}_{n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(features, actual_label, weights):                                                         \n",
    "    prediction = predict(features, weights)    \n",
    "    \n",
    "    d1 = (prediction * (1 - prediction)).T *   np.power(weights,2)        \n",
    "    d2 = (prediction * (1 - prediction)).T *  weights        \n",
    "    d3 = (prediction * (1 - prediction))                \n",
    "    H = np.array([[d1, d2],[d2, d3]])                                           \n",
    "    return H, np.array([[(actual_label - prediction).T * weights,                          \n",
    "                     (actual_label - prediction) * 1]])  \n",
    "\n",
    "def update_weights(features, labels, weights, lr, order):\n",
    "    N = len(features)\n",
    "    predictions = predict(features, weights)\n",
    "    if order == 1:\n",
    "        gradient = np.dot(features.T,  predictions - labels)\n",
    "        gradient /= N\n",
    "        gradient *= lr\n",
    "        weights -= gradient\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign class labels (0 or 1) to our predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(prob):\n",
    "    return 1 if prob >= .5 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the data, and get optimized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, weights, lr, iters, order):\n",
    "    cost_history = []\n",
    "    accracy_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr,order)\n",
    "\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % 1000 == 0:\n",
    "            print(\"iter: \",str(i) , \" cost: \",str(cost))\n",
    "\n",
    "    return weights, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy measures how correct the predictions were. \\\n",
    "In this case we compare predicted labels to true labels and divide by the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = predicted_labels - actual_labels.T\n",
    "    return 1.0 - (float(np.count_nonzero(diff)) / diff.shape[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the first-order gradient descent method on logistic regression, the loss is dropped significantly in the first 500 loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:  0  cost:  0.7318607944902067\n",
      "iter:  1000  cost:  0.284700461694812\n",
      "iter:  2000  cost:  0.1826267114615742\n",
      "the accuracy of the first-order gradient descent is : 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = df.shape[0]\n",
    "n = df.shape[1]-1\n",
    "x = np.zeros((m,n))\n",
    "\n",
    "for i,element in enumerate(feature_cols):\n",
    "    x[:,i] = df[element]\n",
    "\n",
    "y = np.zeros((m,1))\n",
    "y[:,0] = df['desire']\n",
    "\n",
    "#initialize the parameters\n",
    "w = np.random.rand(n,1)\n",
    "lr = 0.01\n",
    "iters = 3000\n",
    "\n",
    "w, cost_history = train(x,y,w,lr,iters,1)\n",
    "predictions = predict(x, w)\n",
    "predict_label = []\n",
    "for i in predictions:\n",
    "    predict_label.append(decision_boundary(i))\n",
    "accuracy = accuracy(predict_label, y)\n",
    "plt.plot(range(len(cost_history)),cost_history)\n",
    "print('the accuracy of the first-order gradient descent is :',accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
